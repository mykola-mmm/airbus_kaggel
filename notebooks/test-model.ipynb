{"cells":[{"cell_type":"markdown","metadata":{"id":"MZu1fURA5jOV"},"source":["config.py:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CD82dQDe5qw5","trusted":true},"outputs":[],"source":["import os\n","\n","# model configs\n","NUM_SAMPLES = 3000  # not a total number of samples, but max number of samples with the same 'ship_count'\n","VALIDATION_SET_SIZE = 0.1 # number from 0 to 1.0\n","GAUSSIAN_NOISE = 0.1\n","NB_EPOCHS = 40\n","BATCH_SIZE = 64\n","PATCH_SIZE = 256\n","INPUT_DATA_DIM = (PATCH_SIZE, PATCH_SIZE, 3)\n","\n","# env_configs\n","BASE_DIR = '/kaggle/input/airbus-ship-detection'\n","TEST_IMG_DIR = os.path.join(BASE_DIR,'test_v2')\n","TRAIN_IMG_DIR = os.path.join(BASE_DIR,'train_v2')\n","TRAIN_DATASET_CSV = os.path.join(BASE_DIR,'train_ship_segmentations_v2.csv')\n","\n","# WEIGHTS_DIR = 'weights'\n","# WEIGHTS_FILE = 'model.{epoch:02d}-{val_loss:.2f}.weights.h5'\n","# WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, WEIGHTS_FILE)\n","\n","MODEL_DIR = 'model'\n","MODEL_FILE = 'model.{epoch:02d}-{val_loss:.2f}.keras'\n","MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n","\n","# model testing configs\n","MODEL_TO_TEST = '/kaggle/input/10h-model/keras/10h/1/model.30-0.35.keras'\n","# MODEL_TO_TEST_PATH = os.path.join(MODEL_DIR, MODEL_TO_TEST)\n","MODEL_TO_TEST_PATH = os.path.join(MODEL_TO_TEST)"]},{"cell_type":"markdown","metadata":{"id":"QjGWfzv159Y3"},"source":["utils.py:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_nz8dxN6A0C","trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","\n","#utility functions\n","def rle_to_mask(starts, lengths, height, width):\n","    # Create an empty array of zeros of shape (height, width)\n","    mask = np.zeros(height * width, dtype=np.uint8)\n","\n","    # For each start and length, set the corresponding values in the mask to 1\n","    for start, length in zip(starts, lengths):\n","        mask[start:start + length] = 1\n","\n","    # Reshape the mask into the desired dimensions\n","    mask = mask.reshape((height, width))\n","    mask = mask.T\n","    return mask\n","\n","def create_mask(mask_array, width=768, height=768):\n","    masks = np.zeros((width, height), dtype=np.int16)\n","    # if element == element:\n","    if isinstance(mask_array, str):\n","        split = mask_array.split()\n","        startP, lengthP = [np.array(x, dtype=int) for x in (split[::2], split[1::2])]\n","        masks += (rle_to_mask(startP, lengthP, width, height))\n","    return masks\n","\n","def generate_prediction(model, img_dir, img_name):\n","    img = os.path.join(img_dir, img_name)\n","    img = Image.open(img)\n","    img = np.array(img)\n","    img = resize(img, (PATCH_SIZE, PATCH_SIZE), anti_aliasing=True)\n","    img = tf.expand_dims(img, axis=0)\n","    pred = model.predict(img)\n","    print(f\"prediction shape - {pred.shape}\")\n","    return pred, img\n","\n","def visualise_prediction(model, img_dir, img_name):\n","    pred, img = generate_prediction(model, img_dir, img_name)\n","    plt.figure(figsize=(10, 10))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(img[0])\n","    plt.title(\"Original Image\")\n","    plt.axis(\"off\")\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(pred[0])\n","    plt.title(\"Predicted Mask\")\n","    plt.axis(\"off\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"l_nzSt3_7vQn"},"source":["losses.py:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qbm9R-Da7xDJ","trusted":true},"outputs":[],"source":["import tensorflow.keras.backend as K\n","\n","# loss functions\n","def dice_score(y_true, y_pred, smooth=1e-6):\n","    intersection = K.sum(y_true * y_pred, axis=[1,2,3]  )\n","    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n","    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n","\n","\n","def dice_loss(y_true, y_pred):\n","    return 1 - dice_score(y_true, y_pred)\n","\n","def bce_loss(y_true, y_pred):\n","    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=[1, 2, 3])\n","\n","def dice_bce_loss(y_true, y_pred):\n","    dice_loss_value = dice_loss(y_true, y_pred)\n","    bce_loss_value = bce_loss(y_true, y_pred)\n","    return dice_loss_value + bce_loss_value"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow import keras\n","import random\n","\n","\n","# Handpicked images for nice results :)\n","handpicked_test_imgs = ['ba6fe8a25.jpg','8dd7d6368.jpg','534e10e90.jpg','711be1fce.jpg','8e6b39bb1.jpg','45d43380c.jpg','2868e5640.jpg','a00886458.jpg','0c2848844.jpg','d9b95022a.jpg','ba9c3d11a.jpg','3566fb758.jpg','abb672b82.jpg','acb7dd8d2.jpg', '30e126c21.jpg']\n","\n","# choose 100 random images from test directory for unbiased results\n","file_names = os.listdir(TEST_IMG_DIR)\n","test_imgs = random.sample(file_names, 33)\n","\n","model = keras.models.load_model(MODEL_TO_TEST_PATH, custom_objects={'dice_bce_loss': dice_bce_loss, \"dice_score\": dice_score})\n","\n","for i in handpicked_test_imgs:\n","   visualise_prediction(model, TEST_IMG_DIR, i) \n","\n","for i in test_imgs:\n","   visualise_prediction(model, TEST_IMG_DIR, i) "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":868324,"sourceId":9988,"sourceType":"competition"},{"isSourceIdPinned":true,"modelInstanceId":64999,"sourceId":77306,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
